Concept:can 

- There are 4 top-level "sections" - variables, sensors, actuators, and behaviors
- Parameters are quoted strings
- They are either "/xxx" for a topic or ":yyy" for a blackboard variable
- Most of the parameters have good defaults so that they often can just be left out
-

variables {

}

sensors {
    lidar pub: "/scan",  model: "ydlidar
    filter type: "remove_noise", from: "/scan", to: "/scan_filtered"
    detector type: "target", from: "/scan_filtered", bearing: ":target_bearing", ":target_distance"
	detector type: "cv", target: "bottle", image: "/camera/image_filtered"
    detector "collision", from: "/scan_filtered", to: ":distance_to_obstacle"
    camera pub: "/camera/image_raw", model: "picamera"
}

The parent nodes can take in parameters detailing the sensors and filters that will 
be passed down to all child nodes. You do not have to specify redundant sensors/filters
in child nodes whose parents already have the sensors/filters specified.
Sequencer {
	sensors: "lidar"
	filters: "fov":45, "range":0.5, "angle":0
	children:[
		Sequencer {
			children:[

			]
		}
	]
}

behavior {
	sensors: [ lidar: { pub: "/scan_front" },
				camera: "enabled"
	]
	filters: [
		lidar: { "fov":45, "range":0.5, "angle":0, sub: "/scan_front", pub: "/scan_filtered" },
		camera: { "downsample: "25%", greyscale: "true" }
		obstacle_detect { "sub": "/scan_filtered","distance": ":distance_to_obstacle" , "bearing: ":target_bearing", "detected": ":detected" }
	]
	selector:[
		sequencer:[
			conditional:"close to obstacle"{sub:":detected"}
			action:"stop all movement"
		],
		action:"move towards bearing" {sub:":target_bearing"}
		
		# am I close to an obstacle?, if so stop?
		# otherwise: drive towards obstacle

		}
	]
}


actuators {
    movement speed: ":meter_per_sec", turn: ":rad_per_sec"
	horn seconds: ":horn_sec“
	speech text: ":sentence"
}
behaviors {


    sequence main  {
        move(:target_bearing, :target_distance, :avg_front)
    }

sequence move  {
	preprocess_odom()
	move_to_position()
	pid()
}

# full item follower


# question: can’t the Odom message be automatically destructured into
# BB variables position and rotation?

parallel preprocess_odom {
	position = getPosition(“/odom”)
	rotation = getRotation(“/odom”)
}

# Or do we declare some behavior wide settings:

behavior(“follow_wall”) {
	Use_degrees
}





Blackboard:

## default_move_bearing_liinear_vel = 0.2

Services:
	….


Behaviors:

Sequence main {
	follow_and_avoid()
	follow_wall()
}

Sequence follow_and_avoid {
	clearAhead()
	detection = BoxDetector(“/camera_node”, 
	follow_item_stop_stopper()
}

Sequence follow_wall {
	scan_preprocess()
	pid()
	move_accordning_to_pid()
}

Selector follow_item_stop_stopper {
	sequence stop_item {
		bearing_topper = item_bearing_error(detection, “Bottle”)
		stop()
	}
	Sequence follow_item {
		Bearing_target = item_bearing_error(detection, “face”)
		move_beaing(bearing_target)



If wall_ahead(0.4):
	follow_wall()
Else:
	Detection = run_box_detector()
	If is_item_visible(detection, “bottle”):
		stop()
	Else if is_item_visible(detection, “person”):
		Bearing_target = item_bearing_error(detection, “person”)
		move(bearing_target)






